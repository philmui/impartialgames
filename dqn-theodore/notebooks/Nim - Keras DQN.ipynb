{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0e88181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "# after gym-0.25.2, the \"step\" API requires separate\n",
    "#         terminated vs truncated return boolean \n",
    "__requires__ = ['gym <= 0.25.2']\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import BatchNormalization, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "\n",
    "from rl.core import Processor\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import BoltzmannQPolicy, LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de51e786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checker: obs = [30 40 50], obs_space = MultiDiscrete([31 41 51])\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "# SETUP\n",
    "###########################################################\n",
    "REPLAY_BUFFER_SIZE=100000\n",
    "TRAIN_NB_STEPS=250000\n",
    "PILES = np.array([30, 40, 50])\n",
    "NUM_PILES = len(PILES)\n",
    "AGENT_WEIGHTS_FILE = f\"wts/dqnwts-{'-'.join(PILES.astype(str))}\"\n",
    "\n",
    "memory = SequentialMemory(limit=REPLAY_BUFFER_SIZE, window_length=1)\n",
    "\n",
    "# policy = BoltzmannQPolicy()\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(),\n",
    "                              attr='eps',\n",
    "                              value_max=1.0,\n",
    "                              value_min=0.1,\n",
    "                              value_test=0.05,\n",
    "                              nb_steps=TRAIN_NB_STEPS)\n",
    "\n",
    "from nimenv import NimEnv, MIN_REWARD, MAX_REWARD\n",
    "\n",
    "env = NimEnv(PILES)\n",
    "obs = env.reset(seed=42)\n",
    "print(f\"checker: obs = {obs}, obs_space = {env.observation_space}\")\n",
    "# first check to make sure that the env is in compatible shape\n",
    "from gym.utils.env_checker import check_env\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89150eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/pmui/opt/anaconda3/lib/python3.9/site-packages/keras/layers/normalization/batch_normalization.py:514: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 3)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                256       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 64)               256       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 120)               7800      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,632\n",
      "Trainable params: 16,504\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten(input_shape=(1, NUM_PILES)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(env.action_space.n, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "class NimProcessor(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        # assert len(observation) == len(PILES)\n",
    "        return observation\n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        return batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, MIN_REWARD, MAX_REWARD)\n",
    "\n",
    "class NimDQNAgent(DQNAgent):\n",
    "    def forward(self, observation):\n",
    "        action = super().forward(observation)\n",
    "        count = 0\n",
    "        while not env.done and not env.is_action_valid(action):\n",
    "            action = super().forward(observation)\n",
    "            if ++count > 4: # something is not right to choose so many times\n",
    "                action = env.get_random_action()\n",
    "                print(f\"\\t***> new action: {action}\")\n",
    "                break\n",
    "        return action\n",
    "\n",
    "agent = NimDQNAgent(model=model, \n",
    "                    nb_actions=env.action_space.n, \n",
    "                    memory=memory, \n",
    "                    processor=NimProcessor(),\n",
    "                    nb_steps_warmup=500,\n",
    "                    target_model_update=500, \n",
    "                    batch_size=128, \n",
    "                    gamma=0.99,\n",
    "                    policy=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a0e9f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights at: wts/dqnwts-30-40-50 ...\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "# TRAINING\n",
    "###########################################################\n",
    "agent.compile(optimizer=Adam(learning_rate=1e-3),\n",
    "              metrics=[\"mae\"]) \n",
    "\n",
    "#if os.path.exists(AGENT_WEIGHTS_FILE):\n",
    "print(f\"Loading weights at: {AGENT_WEIGHTS_FILE} ...\")\n",
    "agent.load_weights(AGENT_WEIGHTS_FILE)\n",
    "# except:\n",
    "# agent.fit(env, nb_steps=TRAIN_NB_STEPS, visualize=False, verbose=1)\n",
    "# agent.save_weights(AGENT_WEIGHTS_FILE, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b6d2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======NEW GAME=========\n",
      "[30 40 50]\n",
      "\tAgent action: 76 => [30 40 44] 0 False\n",
      "[30 40 44]\n",
      "\tEnter pile (0-based): 2\n",
      "\tStones to remove: 44\n",
      "\t[30 40  0] 0 False\n",
      "[30 40  0]\n",
      "\tAgent action: 41 => [30 29  0] 0 False\n",
      "[30 29  0]\n",
      "\tEnter pile (0-based): 0\n",
      "\tStones to remove: 30\n",
      "\t[ 0 29  0] 0 False\n",
      "[ 0 29  0]\n",
      "\tAgent action: 35 => [ 0 24  0] 0 False\n",
      "[ 0 24  0]\n",
      "\tEnter pile (0-based): 1\n",
      "\tStones to remove: 24\n",
      "\t[0 0 0] 2 True\n",
      "\tGame over, you win\n",
      "=======NEW GAME=========\n",
      "[30 40 50]\n",
      "\tAgent action: 76 => [30 40 44] 0 False\n",
      "[30 40 44]\n"
     ]
    }
   ],
   "source": [
    "###########################################################\n",
    "# INTERACTIVE PLAY\n",
    "###########################################################\n",
    "\n",
    "def play():\n",
    "    more_game = True\n",
    "    while more_game:\n",
    "        print(f\"=======NEW GAME=========\")\n",
    "        env.reset()\n",
    "        while not env.done:\n",
    "            print(f\"{env.state}\")\n",
    "            action = agent.forward(env.state)            \n",
    "            obs, reward, done, info = env.step(action)\n",
    "            print(f\"\\tAgent action: {action} => {obs} {reward} {done}\")\n",
    "\n",
    "            if env.done:\n",
    "                print('\\tGame over, agent wins')\n",
    "            else:\n",
    "                print(env.state)\n",
    "                input_mode = True\n",
    "                while input_mode:\n",
    "                    try:\n",
    "                        usr_pile = int(input('\\tEnter pile (0-based): '))\n",
    "                        stones_remove = int(input('\\tStones to remove: '))\n",
    "                        user_action = env.get_action(usr_pile, stones_remove)\n",
    "                        if stones_remove != 0 and \\\n",
    "                           env.is_action_valid(user_action):\n",
    "                            obs, reward, done, info = env.step(user_action)\n",
    "                            print(f\"\\t{obs} {reward} {done}\")\n",
    "                            if env.done: print('\\tGame over, you win')\n",
    "                            input_mode = False\n",
    "                        else: \n",
    "                            print('\\tInvalid move: please try again')\n",
    "                    except:\n",
    "                        more_game = False # assume user wants to quit\n",
    "                        input_mode = False\n",
    "                        env.done = True\n",
    "play()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d42e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
